# Pitagora Meetup 2024-01

## 概要

- 日時： 2024年1月11日（木） 13:00 〜 18:00
- 場所：
- オンサイト会場: 貸し会議室 (東京都板橋区前野町3-41-1)
  - 人数制限があるため、オフライン参加を希望する方は Slack にてご連絡ください
  - 感染症対策にご協力ください。
  - オンライン会場:
    - Discord: `Pitagora / Workflow Meetup Japan` サーバ

## 問い合わせ

- 幹事: 大田 `tazro.ohta [at] chiba-u.jp`

## タイムテーブル

途中参加、途中退出は自由です。初めて参加される方はぜひ [過ごし方ガイド](/events/meetup/whatis) をご覧ください。

|    Time     |                        |
| :---------: | :--------------------: |
| 13:00-13:15 |     今日の作業確認     |
| 13:15-17:00 | 開発とディスカッション |
| 17:00-18:00 |      ラップアップ      |
|   18:00-    |       有識者会議       |

## ミートアップの主旨

- データ解析ツール・ワークフローを共有する
  - ソフトウェアをコンテナ化する
  - Galaxy や CWL などを使ってツール定義、ワークフロー定義を書く
  - ドキュメントを書いて GitHub で公開する
- 共有されたツールやワークフローを実行する
  - うまくいったこと、うまくいかなかったことを記録する
- 技術情報を共有する
  - SNS に post する
  - ブログを書く
- 関連OSSプロジェクトにコミットする
  - SNSで質問する
  - プロジェクトの Gitter channel があればそこで聞く
  - Issueを立てる
  - Pull Request を送る
- その他データ解析に関わる人々の日々の暮らしが豊かになる開発を進める

## まとめ

### 大田

- Llama2 via `transformers` on Jupyter Notebook in Docker container on a local machine with NVIDIA GPU をやる
  - かつては nvdia-docker とかいろいろあったけど今は [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit) を使うということらしい
    - [参考](https://medium.com/nvidiajapan/nvidia-docker-%E3%81%A3%E3%81%A6%E4%BB%8A%E3%81%A9%E3%81%86%E3%81%AA%E3%81%A3%E3%81%A6%E3%82%8B%E3%81%AE-20-09-%E7%89%88-558fae883f44) 2020年の記事で既に古いが歴史が書いてある
    - [installation guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
  - CUDA Toolkit はいらないけど NVIDIA driver はインスコしてないとだめだよとのこと
    - apt で入る[とのこと](https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html#ubuntu-lts)
  - 参考:
    - [GPU が使える Jupyter Notebook 環境を最速で用意する](https://qiita.com/kaijism/items/ada49192df0a6d285c3a)
    - [Ubuntu20.04LTS + docker + tensorflow(GPU) + jupyter notebook で環境構築](https://qiita.com/mizuhugu35/items/31dcbb1600d2e77c5d13)
- 身の回りのGPU載ってるマシンを調査
  - 大学のサーバ室のマシンに RTX 6000 ada が載っている
  - ラボの共用クラスタのGPUノードに A40 が 4台刺さっている
  - 遺伝研の V100 4枚刺し GPU ノードを借りることになっている

#### ログ

- WS に構築
- nvidia driver を確認
- 何も入ってないので curl docker を入れる ([参考](https://docs.docker.com/engine/install/ubuntu/))
- nvidia-container-toolkit を入れる ([参考](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-apt))

```
$ nvidia-smi
Wed Jan 10 23:29:35 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX 6000...  On   | 00000000:2D:00.0 Off |                  Off |
| 30%   27C    P8     3W / 300W |     82MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1333      G   /usr/lib/xorg/Xorg                 63MiB |
|    0   N/A  N/A      1480      G   /usr/bin/gnome-shell               16MiB |
+-----------------------------------------------------------------------------+
$ sudo apt update -y && apt install -y curl
$ curl -fsSL https://get.docker.com -o get-docker.sh
$ sudo sh ./get-docker.sh
$ sudo usermod -aG docker $USER
$ docker info
$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
$ sudo apt-get update -y
$ sudo apt-get install -y nvidia-container-toolkit
$ sudo docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu20.04 nvidia-smi
docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]].
```

- だめじゃん！と思ったら nvidia container toolkit を入れたあと dockerd を再起動しないとだめっぽい。
- [jupyter/pytorch-notebook](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#jupyter-pytorch-notebook) といういい感じのコンテナがあった

```
$ sudo service docker restart
$ docker run --rm --gpus all quay.io/jupyter/pytorch-notebook:2024-01-08 nvidia-smi
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX 6000...  On   | 00000000:2D:00.0 Off |                  Off |
| 30%   27C    P8     2W / 300W |     82MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
$ docker run -d --name nb -p 8888:8888 --rm --gpus all quay.io/jupyter/pytorch-notebook:2024-01-08
```

- port forward してローカルで notebook 叩けた、ヨシ！

```
$ docker run --rm -it --gpus all quay.io/jupyter/pytorch-notebook:2024-01-08 bash
$ python3
>>> import torch; torch.cuda.is_available()
False
```

- と思ったら notebook 上の torch がGPUを認識していない
  - ローカルでは動く + nvidia-smi は動くので jupyter/pytorch コンテナ内の cuda version の問題と思われる
  - これを解決するのは微妙にめんどくさそうな気がする。。
- 物理GPUとネイティブ nvidia driver とコンパチな cuda のnvidia公式コンテナイメージを持ってきて、そこに jupyter 等々をｲﾝｽｺしていく方向で

```
$ cat Dockerfile
FROM nvidia/cuda:12.0.0-base-ubuntu20.04

USER root

COPY ./requirements.txt /tmp
WORKDIR /code

RUN apt update -y && apt upgrade -y && apt install -y curl python3 python3-distutils
RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python3 get-pip.py

RUN pip install -r /tmp/requirements.txt
$ cat requirements.txt
jupyter
jupyterlab
numpy
pandas
matplotlib
scikit-learn
scikit-image
scipy
torch
torchvision
transformers
$ docker build . -t  gpu-notebook:20240111
$ docker run --rm -it --gpus all gpu-notebook:20240111 bash
$ python3
>>> import torch; torch.cuda.is_available()
True
```

todo

- notebook 上から gpu 叩けるかチェック
- torch で gpu 叩けるかチェック
- huggingface で llama2 のモデルを落としてくる
- llama2 を notebook で叩く

### 露崎

- 大目標
  - PyTorch × GPUで機械学習モデリング
  - → Pythonパッケージ開発
  - → 共同研究に応用（メタボローム、患者データ等）
- 中目標
  - そのためのGPU環境を整備（2台ある）
  - 以下を動作させたい
  ```
  python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.current_device()); print(torch.cuda.device_count())"
  ```
  - cf. うまくいかないときのエラー
  ```
  Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/home/koki/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py", line 769, in current_device
    _lazy_init()
  File "/home/koki/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py", line 289, in _lazy_init
    raise AssertionError("Torch not compiled with CUDA enabled")
  AssertionError: Torch not compiled with CUDA enabled
  ```
  - [to()メソッド](https://atmarkit.itmedia.co.jp/ait/articles/2008/28/news030.html)でCPU → GPUに切り替えて、計算が高速になるのを体感したい
- 方針
  - 仮想環境で少しでも再現性のある環境構築（`mamba`、もしくは`virtualenv`）
- 必要なもの
  1. **NVIDIA driver**（最初から入ってた、バージョンが大事らしい）
  - 確認方法: `nvidia-smi`
  ```
  Thu Jan 11 14:17:21 2024
  +---------------------------------------------------------------------------------------+
  | NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |
  |-----------------------------------------+----------------------+----------------------+
  ...
  ```
  2. **CUDA**（最初から入ってた、バージョンが大事らしい）
  - 確認方法: `nvcc -V`
  ```
  nvcc: NVIDIA (R) Cuda compiler driver
  Copyright (c) 2005-2023 NVIDIA Corporation
  Built on Tue_Jun_13_19:16:58_PDT_2023
  Cuda compilation tools, release 12.2, V12.2.91
  Build cuda_12.2.r12.2/compiler.32965470_0
  ```
  3. **Python**（3.11）
  4. **Pythonパッケージ**
  - _Conda-forge系_: tensorboard, pandas, matplotlib, python-graphviz, pot
  - _Anaconda系_: jupyterlab
  - _PyTorch系_: pytorch, torchvision, torchaudio
  - _NVIDIA系_: pytorch-cuda
- ~~課題~~（今日解決した）
  - `mamba create`でまとめてツールをインストールしようとすると1日がかり（チャンネル数が多いせい？[キャッシュ](https://weblabo.oscasierra.net/python-anaconda-clean/)が溜まってる？）
  ```
  mamba create -n pytorch python=3.11 pytorch torchvision torchaudio tensorboard pandas matplotlib jupyterlab python-graphviz pot -c conda-forge -c anaconda -c pytorch -c nvidia -y
  ```
  - 以下は動く（最小限の構成）
  ```
  mamba create -n test pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
  ```
  - pytorch-cudaのバージョンをミスって出たエラー
  ```
  Could not solve for environment specs
  The following package could not be installed
  └─ pytorch-cuda 12.2**  does not exist (perhaps a typo or a missing channel).
  ```
- わかったこと
  1. 簡素に`mamba create` → その中で`mamba install`の方が早い
  ```
  conda activate test
  mamba install -c conda-forge tensorboard -y
  mamba install -c conda-forge pandas -y
  mamba install -c conda-forge matplotlib -y
  mamba install -c anaconda jupyterlab -y
  mamba install -c conda-forge python-graphviz -y
  mamba install -c conda-forge pot -y
  ```
  2. 最初から[本家ドキュメント](https://pytorch.org/get-started/previous-versions/)を参照すべし（個人ブログの情報はノイズが多い）

### 池田

- apache sparkをクラスター上で実行するため

  - hadoopのインストールをひたすら実行していました
  - いろいろ難しい...

  超高速なファイルシステムがあれば、HDFSを利用しなくても共有ファイルに対してsparkを実行すればよいよ(遺伝研ではそうしている[丹生])

### 那須野

- 先月Meetupで試した kubespray.io のおさらい
- RIKEN-RCCSの [WHEEL](https://github.com/RIKEN-RCCS/WHEEL) （Webベースのワークフローツール）を K8s にデプロイしてみたい！
  - WHEELサーバからリモートの計算資源（ジョブ実行環境）へは、入出力ファイルはrsyncで転送、sshでスクリプト実行するシンプルな仕組み
  - 富岳のOpen OnDemand環境では、SingularityコンテナでWHEELが起動する方式で運用しているらしい

### 福井

- 仕事をした
- [`tataki`](https://github.com/sapporo-wes/tataki) :　バイオインフォマティクス関連ファイルのファイル形式を判定するコマンドラインツール。(i.e. `hoge.fa`は本当にFASTAなのか、を調べる)
  - [`tataki`](https://github.com/sapporo-wes/tataki)の仕様をdiscussionした
    - 設定ファイル (`tataki.conf`) の書き方
    - 出力フォーマット
    - [edamontology](https://github.com/edamontology/edamontology/releases)内の`EDAM_1.25.csv`を改変して`tataki`側に組み込む際のライセンスを確認。[`CC BY-SA 4.0 DEED`](https://creativecommons.org/licenses/by-sa/4.0/deed.en)の`ShareAlike`あるけど引用をちゃんと記述すれば大丈夫そう。

### 末竹

- Sapporo の RO-Crate 機能に MultiQC を統合
  - Tonkaz の比較に MultiQC Report の生データを使う
- workflow input parameter の schema を jsonschema で書かせよう問題
  - 現状 cwltool --make-template で template は出てくるが、僕的にほしいのは template ではなく schema
  - tanjo-san にいくつか test に使えそうな cwl file の eg をもらった
    - 定義自体が意地悪なケース:
      - https://github.com/common-workflow-language/cwl-v1.2/blob/main/tests/echo-tool-packed.cwl
      - https://github.com/common-workflow-language/cwl-v1.2/blob/main/tests/revsort-packed.cwl
    - 型が意地悪なケース:
      - https://github.com/common-workflow-language/cwl-v1.2/blob/main/tests/anon_enum_inside_array.cwl
    - パラメータ数が少し多くて、定義そのものは素直なケース
      - https://github.com/common-workflow-language/cwl-v1.2/blob/main/tests/bwa-mem-tool.cwl
    - CommandInputParameter が短縮形 (e.g., param: string) になっているケース
      - https://github.com/common-workflow-language/cwl-v1.2/blob/main/tests/env-tool1.cwl
    - 入力パラメータなし
      - https://github.com/common-workflow-language/cwl-v1.2/blob/main/tests/envvar3.cwl
    - Any
      - https://github.com/common-workflow-language/cwl-v1.2/blob/main/tests/params.cwl
  - cwl-utils に PR をつくってみる

### 丹生

- CWL v1.2.1 が[リリースされた](https://github.com/common-workflow-language/cwl-v1.2/releases/tag/v1.2.1)
  - 公式のアナウンスが [X](https://twitter.com/commonwl) から Mastodon ([floss.social](https://floss.social/@cwl)) に移行したため、X だけだと発見できない
- 遺伝研内で、rootless podman 全面導入できないの？というネタ振りをした
  - rootless, daemonless, SIF も動く
  - これでいいのでは？
- 論文の構成を考えていた

  - 素敵なタイトルを考える力がほしい

- [Kubernetes in Rootless Podman.pdf](https://github.com/AkihiroSuda/AkihiroSuda/blob/master/slides/2023/20231116%20%5BPodman%20Special%20Event%5D%20Kubernetes%20in%20Rootless%20Podman.pdf)　何か読もうと思って放置してた関連資料あるのでシェアします　by　福井

### 山本

- 大きな目的
  - [NCGM-genome/WGSpipeline](https://github.com/NCGM-genome/WGSpipeline/tree/cwl-to-nf)をNextflowに変換
- 小さな目的
  - 1step（process）をNextflowに変換
- CWLのコードをNextflowに変換
  - 変換対象のCWLについて整理✅
    - GitHub
      - [NCGM-genome/WGSpipeline](https://github.com/NCGM-genome/WGSpipeline/tree/cwl-to-nf)
    - コードファイル
      - Workflows/germline-gpu.cwl
      - ①Tools/pbrun-fq2cram-multiRGs.cwl
      - ②Tools/pbrun-haplotypecaller-from-cram.cwl
    - steps数（process数）
      - 1. Fq2Cram：①のTool使用
      - 2. haplotypecaller_autosome：②のTool使用
      - 3. haplotypecaller_PAR：②のTool使用
      - 4. haplotypecaller_chrX_female：②のTool使用
      - 5. haplotypecaller_chrX_male：②のTool使用
      - 6. haplotypecaller_chrY：②のTool使用
    - 入力情報
      - [コマンドライン引数](https://github.com/NCGM-genome/WGSpipeline/tree/cwl-to-nf?tab=readme-ov-file#tutorial-1-run--germline-gpucwl-workflow-with-a-pair-of-fastq-files)
      - arguments
    - コンテナ（singularityコマンドで実行）
      - ①のTool：hacchy/pbrun-fq2bam:4.0.0-1_v20230412
      - ②のTool：nvcr.io/nvidia/clara/clara-parabricks:4.0.0-1
    - マシンの設定
      - hints:でCUDAの条件を指定している
  - 理解したこと✅
    - .nfとnextflow.configを用意
    - .nfのprocess数は6つ
    - nextflow.configで入力情報、singularityの指定、コンテナのリポジトリを指定
  - わからないこと✅
    - 入力値
      - paramsの書き方
      - コンテナ
        - singularityの指定の仕方（リポジトリはDockerHub
      - ワークフロー
        - Nextflowのworkflowフィールド（処理の実行順）の書き方
  - Nextflowの公式Documentを読む🏃
    - https://www.nextflow.io/docs/latest
  - Nextflowのチュートリアルに取り組む
    - https://training.nextflow.io/basic_training
  - わからないことを再考
  - 1step（process）をNextflowに変換
    - Fq2Cramのstepを変換
- メモ
  - (by suecharo) 昔考えたことありますけど、LLM に食わせて変換させて、かつ変換がちゃんと出来ているかを確認するための test を用意したほうが楽だったと思います
    - (by yamaken)たしかに、LLMに食わせて変換はやっていましたが（うまくいかなかった）、テストを自前で用意するのはしてなかった。
  - Nextflowについての違和感：input outputの依存関係で処理の順番決めてよさそうなのに、「workflow」で定義しないといけないところ
- 余談
  - 年末に副鼻腔炎の手術した。鼻詰まりに悩まされている人はおすすめです。
